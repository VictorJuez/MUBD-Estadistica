---
title: "MBD - Estadística - Práctica II (Clustering)"
output: github_document
---

Víctor Juez

Diciembre 2020

- Enunciado: [practica-2.pdf](./practica-2.pdf)
- Script resultado: [script.R](./script.R)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, 
                      cache=TRUE, 
                      results=FALSE, 
                      message=FALSE, 
                      warning=FALSE)

library(scatterplot3d)  
library(flexclust)
library(NbClust)        # NbClust
library(cluster)
library(factoextra)     # fviz_***
library(kernlab)        # kkmeans
library(clValid)        # clValid
library(cluster)        # pam
library(Amelia)
```

# Introducción
Actualmente, los teléfonos móviles almacenan una gran cantidad de datos en tiempo real sobre nuestras actividades rutinarias. Entre otros parámetros recogen información de nuestra movilidad gracias a sensores integrados dentro del mismo dispositivo. La compañía de teléfonos móviles SAMSAPPLE quiere clasificar la actividad de los usuarios de sus dispositivos en 6 niveles en base a la información recibida en tiempo real. Aunque es un problema que tienen bastante resuelto, han realizado un experimento con 21 voluntarios, los cuales reportaban en cada instante su estado real de actividad categorizado en 6 niveles: tumbado (laying); sentado (sitting); de pie (standing); caminando (walk); bajando escaleras (walkdown); o subiendo (walkup).

## Objetivo
1. Agrupar las distintas respuestas de los sensores procurando mantener las máximas similitudes entre clústeres y la máxima heterogeneidad entre los mismos para discernir el número de tipos de actividades posibles (=número de grupos). Para llevarlo a cabo, se usará la técnica de clustering no supervisado del k-means. Sólo usar los datos de entrenamiento sin usar variable respuesta para hallar el número de grupos.

2. Construir un modelo predictivo que sea capaz de clasificar los individuos en cada instante en una de las 6 categorías de actividad. Para realizar la predicción sobre los datos de test, se deberán usar algunos (o preferentemente todos) los algoritmos vistos en las sesiones: KNN, Naive Bayes, Conditional Trees, Random Forests y SVM. Usar datos de entrenamiento para construir el modelo y hacer las predicciones sobre el conjunto de test

# Parte I. Clustering no supervisado
Para identificar el número de clústers hemos utilizado el K-means. Sabemos que en el conjunto de datos final hay 6 categorías de actividad diferentes, así que, lo ideal sería identificar 6 clústers en el conjunto de datos. Para ello hemos utilizado por un lado el método de la regla del codo y por otro, la librería NbClust que utilizando diferentes índices determina cual es el número de clústeres ideal.

## 1. Regla del codo
Ejecutamos el k-means 10 veces utilizando de 1 a 10 clústeres respectivamente, y por cada resultado comparamos la variabilidad explicada.

```{r}
datos <- read.table('Datos de entrenamiento.txt',header=TRUE,sep='\t', dec = '.')
datos2 = datos;
datos2$subject = NULL;
datos2$activity = NULL;

VE <- c()
for (k in 1:10){
  km <- kmeans(datos2,centers=k,nstart=10)
  VE[k] <- km$betweenss/km$totss       
}
plot(VE,type="b",pch=19,xlab="Número de grupos",ylab="Variabilidad explicada")
```

Como podemos observar, según la regla del codo nos quedamos con dos clústers, ya que, es en este punto donde se forma el codo y, a partir de dos para arriba, el incremento de variabilidad explicada es muy pequeño. A continuación podemos observar la variabilidad explicada por cada número de clústers.

| Num. Clusters | Variabilidad explicada |
|---------------|------------------------|
| 1             | 0.00                   |
| 2             | 0.57                   |
| 3             | 0.61                   |
| 4             | 0.64                   |
| 5             | 0.66                   |
| 6             | 0.67                   |
| 7             | 0.68                   |
| 8             | 0.69                   |
| 9             | 0.70                   |
| 10            | 0.70                   |

Aunque la variabilidad explicada sí que va aumentando a medida que se incrementan el número de clústers, vemos que el incremento más grande se produce cuando se pasa de un clúster a dos.


## 2. NbClust
La librería NbClust utiliza 30 índices diferentes variando por cada uno todas las combinaciones de número de clústers, tipo de distancias y métodos de clusterización para determinar el número de clústers que más encaja al conjunto de datos. 

El coste computacional de esta operción es bastante elevado, y por este motivo hemos reducido la dimensionalidad del conjunto de datos de la siguiente manera:

- **Principal Component Analysis (PCA)**: Nos hemos quedado con las 10 componentes principales en vez de utilizar las más de 500 características que hay en el conjunto de datos.
- **Reducción de observaciones**: Hemos utilzado 500 observaciones en vez de las aproximadamente 5000 que hay en el conjunto de datos.

El resultado obtenido tras la ejecución es el siguiente:

```{r fig.show='hide'}
## ACP previo al análisi de clusters NbClust. 
# Se hace debido al alto coste computacional de la funcion
pr.comp <- princomp(datos2)
#screeplot(pr.comp,type='lines')

# Analisi de clusters
set.seed(12345)
ncluster = NbClust(pr.comp$scores[1:500, 1:10], min.nc=2, max.nc=10, method="kmeans")
```

```{r}
par(mfrow=c(1,1))
barplot(table(ncluster$Best.n[1,]),xlab="Número de grupos",ylab="Número de índices")
```

Por mayoría, el mejor número de clústers es 2, apoyado por 12 índices.

## 3. Representación gráfica y análisis del resultado
Utilizando el mismo Principal Component Analysis (PCA) que hemos realizado anteriormente, hemos extraído los dos componentes más significativos. Éstos van a formar el eje X e Y respectivamente, y sobre ellos mostramos el conjunto de datos marcados con colores distintos para representar el clúster al que pertenecen.

```{r}
##-- 2 grupos
km2 <- kmeans(datos2,centers=2,nstart=10)
ve2 <- km2$betweenss/km2$totss

##-- En las 2 primeras componentes
pr.comp <- princomp(datos2)
x <- pr.comp$scores[,1]
y <- pr.comp$scores[,2]
plot(x,y,pch=19,col=km2$cluster, xlab="Componente 1",ylab="Componente 2")
```

Vemos que la partición en dos clústers es coherente y que a simple vista no se identifican más que éstos dos. Esto nos indica que utilizando el conjunto de datos de muestra, no podemos discernir con presición entre las 6 actividades diferentes. Por este mismo motivo, observamos que la variabilidad explicada utilizando únicamente dos clústers es de 0.57, un valor bajo, y es que estamos dejando de identificar 4 categorías.

# Parte II. Clustering supervisado
Hemos dividio el conjunto de datos en dos muestras: 

- Muestra de entrenamiento, para entrenar los modelos. Representa un 70% del conjunto de datos
- Muestra de test, para analizar el rendimiento de los distintos algoritmos que utilizaremos. Representa un 30% del conjunto de datos.

## 1. K-Nearest Neighbor
### Buscar la K óptima

```{r}
library(deldir)
library(kknn)
library(class)

allPredictions = data.frame(
  'algorithm' = c('KNN', 'Bayes', 'Conditional Trees', 'Random Forests', 'SVM'),
  'prediction capacity' = c(0,0,0,0,0)
)

d = read.table('Datos de entrenamiento.txt',header=TRUE,sep='\t', dec = '.', stringsAsFactors = TRUE)
d$subject = NULL

##-- Dividir en muestra de entrenamiento y muestra test
p <- 0.7                 # Proporcion en muestra de entrenamiento
n <- dim(d)[1]           # numero de observaciones 
set.seed(12345)
train.sel <- sample(c(FALSE,TRUE),n,rep=TRUE,prob=c(1-p,p))
train <- d[train.sel,]
test <- d[!train.sel,]
```

Primero, ejecutamos el algoritmo utilizando diferentes valores de (`K`) para encontrar el que mejor se ajusta.

```{r}
p <- c()
K <- seq(1,21,2)
for(k in K){
  knn <- knn(train[,-ncol(d)], test=test[,-ncol(d)], cl=train$activity, k = k)
  t <- table(knn,test$activity)
  p[(k+1)/2] <- sum(diag(t))/sum(t)
}
plot(K,p,pch=19,type='b')
```

Vemos que `K = 3` es el que mejor resultado da, con un `0.9622016` de capacidad predictiva.

### Usar la K óptima

```{r echo=TRUE}
knn <- knn(train[,-ncol(d)], test=test[,-ncol(d)], cl=train$activity, k = 3)
```

- Matriz de confusión:

```{r results=TRUE}
t <- table(knn,test$activity)
t
```

- Capacidad predictiva: `0.9622016`.
- Capacidad predictiva por clase:

```{r}
barplot(diag(prop.table(t,2)))
```

A continuación hay dos variantes que probamos para intentar mejorar el resultado sin éxito.

### Variante 1. ACP previo a KNN
Reducimos la dimensionalidad del conjunto de datos previo a ejecutar el KNN y cogemos las primeras 10 componentes principales.

```{r echo = TRUE}
res.acp <- pr.comp$scores[,1:50]
train2 <- res.acp[train.sel,]
test2 <- res.acp[!train.sel,]

K <- seq(1,21,2)
p <- c()
for(k in K){
  knn <- knn(train2, test2, cl=train$activity, k = k)
  t <- table(knn,test$activity)
  p <- c(p,sum(diag(t))/sum(t))
}
plot(K,p,pch=19,type='b')
```

Mejor resultado con `K = 7` que da un `0.9429708` de capacidad predictiva. No mejora.

### Variante 2. Kernel KNN
```{r echo = TRUE}
p <- c()
K <- seq(1,21,2)
for(k in K){
  kknn <- kknn(factor(activity)~., train, test,k=k)
  t <- table(fitted(kknn),test$activity)
  p <- c(p,sum(diag(t))/sum(t))
}
plot(K,p,pch=19,type='b')
```

Mejor resultado con `K = 15` que da un `0.9522546` de capacidad predictiva. No mejora.

## 2. Naive Bayes

```{r}
library(e1071)
library(ineq)

cor.matrix <- cor(d[,-c(1,length(d))])              # Matriz de correlaciones
cor.num <- as.numeric(cor.matrix)                           # Todas las correlaciones en un vector
t.cor <- table(cut(cor.num[cor.num!=1],br=seq(-1,1,0.1)))/2 # Categorazion de las correlaciones en intervalos de 0.1
#barplot(t.cor)
```

```{r echo = TRUE}
nb <- naiveBayes(activity ~ ., train)
```

- Matriz de confusión:

```{r results=TRUE}
preds <- predict(nb, newdata = test)
t <- table(preds, test$activity)
p.acierto <- sum(diag(t))/sum(t)
t
```

- Capacidad predictiva: `0.8023873`. Inferior al KNN.
- Capacidad predictiva por clase:

```{r}
barplot(diag(prop.table(t,2)))
```

Vemos un resultado peor que en el KNN, sobretodo en la predicción de la clase `sitting`.

## 3. Árboles condicionales

```{r}
library(randomForest)
library(party)
library(e1071)
```


```{r echo=TRUE}
ct.mod <- ctree(activity ~ ., train)                                  
```

- Matriz de confusión:

```{r results=TRUE}
pred <- predict(ct.mod,test,type="response")                          
t <- table(pred,test$activity)                                       
p.acierto = sum(diag(t))/sum(t)
t
```

- Capacidad predictiva global: `0.9124668`
- Capacidad predictiva por clase:

```{r}
barplot(diag(prop.table(t,2)))
```

## 4. Random Forest
 
```{r}
set.seed(12345)
```
 
```{r echo=TRUE}
rf.mod <- randomForest(activity~.,train,importance=TRUE,ntree=100,do.trace=TRUE)  
```

- Parámetros especificados:
  - `importance=TRUE`: para que considere la importancia de los predictores.
  - `ntree=100`: número de árboles generados.
  - `do.trace=TRUE`: emite más "logs" para tener más información de la ejecución del algoritmo.

- Resultado del algoritmo:
  - Variables utilizadas por cada árbol generado: 23
  - Out-of-bag error estimado: 3.52%

- Matriz de confusión:

```{r results=TRUE}
pred.rf <- predict(rf.mod,test)
t <- table(pred.rf,test$activity)                         # tabla de predicciones vs respuesta real
p.acierto = sum(diag(t))/sum(t)
t
```

- Capacidad predictiva global: `0.9602122`
- Capacidad predictiva por clase:

```{r}
barplot(diag(prop.table(t,2)))
```

Validamos que no necesitamos generar más árboles:

```{r}
plot(rf.mod, type="l", main="")
```

Vemos que los errores estan estabilizados tras 100 árboles generados, por lo que utilizar más no cambiaría el resultado.

### Variante 1. Valor MTRY óptimo

Buscamos el valor óptimo de `mtry`, el cual especifica la cantidad de variables a utilizar por cada árbol que se genera.

```{r}
mtry.par <- tuneRF(d,d$activity)
set.seed(12345)
```

Dado el gráfico resultante, `mtry = 92` es el óptimo. Volvemos a ejecutar el Random Forest con este parámetro especificado.


```{r echo=TRUE}
rf.mod1 <- randomForest(activity~.,train,importance=TRUE,ntree=100,do.trace=TRUE,mtry=92)
```

- Parámetros especificados: los mismos que antes pero añadiendo el `mtry`.
- Matriz de confusión:

```{r results=TRUE}
pred.rf1 <- predict(rf.mod1,test)
t <- table(pred.rf1,test$activity)
t
#sum(diag(t))/sum(t)   
```

- Capacidad predictiva global: `0.9502653`. No mejora

## 5. Support Vector Machines (SVM)
### Buscar parámetros óptimos

En primer lugar, buscamos qué combinación de los siguientes parámetros nos da mejor resultado con el algoritmo. Los valores a probar son:

- Kernel: `linear, polynomial, radial, sigmod`.
- Coste: `0.01, 0.2, 0.1, 1, 5, 10, 100`.

Para hacerlo, usamos la función `tune()`, la cual en todas las ejecuciones aplica un `Cross Validation (10-fold)`, por lo que no tenemos que utilizar las particiones `train` y `test`, y en cambio, podemos utilizar todo el conjunto de datos directamente.

Cabe mencionar que dado el alto coste computacional de esta operación, volvemos a hacer una reducción de dimensionaliad de los datos. Utilizamos el `Principal Component Analysis (PCA)` realizado anteriormente. 

- Nos quedamos los 10 primeros componentes en vez de las más de 500 características presentes en el conjunto de datos.
- Mantenemos el mismo número de muestras.

```{r echo=TRUE}
# Reducción de dimensionalidad
pr.comp = princomp(d[,-ncol(d)])
d2 = as.data.frame(pr.comp$scores[,1:10])
d2$activity = d$activity[1:nrow(d2)]

# SVM
mod.tune <- tune(svm,activity~.,
                  data = d2,
                  ranges = list(kernel = c('linear','polynomial','radial','sigmoid'),
                                cost = c(0.01,0.2,0.1,1,5,10,100)))
```

```{r echo =TRUE, results= TRUE}
mod.tune$best.parameters
```

De todas las combinaciones, `kernel = radial` y `cost = 10` es la que mejor resultado nos da, con un error de `0.0844`.

### Usar los parámetros óptimos
Ahora que tenemos los parámetros óptimos ejecutamos el SVM utilizando las muestras `train` y `test` y sin reducir su dimensionalidad.

```{r echo=TRUE}
mod.svm <- svm(activity~.,data = train, cost=10, kernel='radial')
```

- Matriz de confusión

```{r results=TRUE}
pr <- predict(mod.svm,test)
t <- table(pr,test$activity)
t
p.acierto = sum(diag(t))/sum(t)
```

- Capacidad predictiva global: `0.9801061`.
- Capacidad predictiva por clase:

```{r}
barplot(diag(prop.table(t,2)))
```

## Tabla de resultados y predicción final

| Algoritmo                 | Capacidad predictiva  |
|----------------------|---|
| KNN           | 0.9622016  |
| ACP + KNN           | 0.9429708  |
| KKNN           | 0.9522546  |
| Naive Bayes  | 0.8023873  |
| Conditional Trees | 0.9124668  |
| Random Forest | 0.9602122  |
| Random Forest + MTRY | 0.9502653  |
| SVM | 0.9801061  |

Dado que el SVM ha sido el algoritmo con el que mayor capacidad predictiva hemos obtenido, es el que hemos utilizado para realizar la predicción final:

- Algoritmo: SVM
- Parámetros:
  - `cost = 10`
  - `kernel = radial`